---
layout: post-wo-sidebar
title: Toward Multimodal Image-to-Image Translation
date: 2018-03-21 15:47:20 +0300
description: 
img: teaser.jpg
content_type: gan
tags: [GAN, Video Generation]
---

## Code Walkthrough for Bi-Cycle GAN  


List of files 

1. bcgan.py
2. layers.py
3. networks.py
4. process_data.py
5. test.py
6. train.py

Libraries

import tensorflow as tf
Code is written in python using tensorflow library.

Other dependencies
numpy, scipy, os, argparse, tqdm, h5py, time, random.


Data preprocessing

file process_data.py
modules - get_data
dependencies -

downloaded data is loaded using this module

augmentation is done on the fly - not part of this step.




Model definitions

file : bcgan.py

dependencies - from network import generator, discriminator, encoder

module : Bicycle_GAN

module is a class definition - for bicyclic gan

functions:
constructor
summary_create
train
test

function: constructor
1. creates all the necessary variable in the class object.
2. uses the modules generator, discriminator and encoder to create 
cVAE-GAN
cLR-GAN
3. formulates the loss functions
4. optimizers
5. update ops(taking care of batchnorm updates)

function: summary_create
1. create the tensorboard summaries for all costs, and images
2. merging all summaries 

function: train
1. runs the main training loop
2. loss minimization and gradient updates 
3. learning rate is periodically decayed 
4. summaries are periodically written

function: test
1. loads the pretrained weights
2. generates the images by random sampling 
3. saves the images 




file network.py
modules - generator, discriminator, encoder
dependencies - from layers import * ( wrapper functions for all the layers)

module - generator

for creating the generator graph definition, with all the conv layers, normalizations, and activations.
returns the final layer output 

functions 
__init__
__call__

module - generator

for creating the discriminator graph definition, uses the deconv layers in addition to other layers to increase the spatial size.
returns the final layer output 

functions 
__init__
__call__

module - encoder

for creating the encoder graph definition, uses the residual skip connections along with other layers.

returns the final layer output 

functions 
__init__
__call__

file layers.py
modules - conv2d, flatten, residual etc â€¦

wrapper functions on top of the tensorflow implementations of the defined layers.

file train.py
modules - collect_args, validate_args, train
dependencies - Bicycle_GAN, get_data

function: collect_args
collect the model parameters and training parameters using the argparse 

function: validate_args
validates the collected arguments are allowable values

function: train

1. sets up the GPU environment and variables
2. loads the training data
3. creates the BiCycle GAN model definition
4. load the pretrained weights if exists
5. call the training function in Bicycle_GAN


file test.py
modules - collect_args, validate_args, train
dependencies - Bicycle_GAN, get_data

function: collect_args
collect the model parameters and training parameters using the argparse 

function: validate_args
validates the collected arguments are allowable values

function: test
1. sets up the GPU environment and variables
2. loads the testing data
3. creates the BiCycle GAN model definition
4. load the pretrained weights
5. call the test function in BiCycle_GAN





## One Line Summary
* Generative Adversarial Network is used for decomposing the motion and content informaiton in the video, and the latent space walk is used for the video generation.

## Motivation
* Motion and object information are inherently very less correlated, for example same object can have different motions, and different objects can have the same motion. 


## Detailed Summary
*  The proposed framework generates a video by mapping a sequence of random vectors to a sequence of video frames. Each random vector consists of a content
part and a motion part. While the content part is kept fixed, the motion part is realized as a stochastic process. To learn motion and content decomposition in an unsupervised manner, this paper introduces a novel adversarial learning scheme utilizing both image and video discriminators.

## Novelty and Contributions
* Representing the motion as a path in the latent space rather than as a point, so that the videos of different length can be represented.


## Network Details
![Network]({{site.baseurl}}/assets/img/adfasdfads.gif)

* LSTM based generative model that uses the encoded motion and the content information for the video generation
![Network]({{site.baseurl}}/assets/img/fdsfafafads.png)

## Results
![Results]({{site.baseurl}}/assets/img/fdsfsdfsa.png)


![Results]({{site.baseurl}}/assets/img/fdsfsdfsdfds.png)

## Authors
Jun-Yan Zhu, Richard Zhang, Deepak Pathak, Trevor Darrell, Alexei A. Efros, Oliver Wang, Eli Shechtman

## Sources
[Paper](https://arxiv.org/abs/1711.11586)

